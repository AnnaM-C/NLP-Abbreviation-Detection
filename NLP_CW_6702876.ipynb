{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Na4JHdou8VdM",
        "Ox0qnyPp8bR8",
        "BFrAvcRnY-_g",
        "Kc5UduIkZF5O",
        "4SfZKE5hZSGm",
        "6DqLdes4ZZxY",
        "7GZkJFkWZntg",
        "kFEJDs-vaERT",
        "awiFCOiZaP97",
        "lULReFH9a_mm",
        "oUOf4MVSWCHD",
        "3HDoaB5fXrlY",
        "jsaM5Ng1WasQ",
        "G5NooaLqc6LD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment Set Up"
      ],
      "metadata": {
        "id": "Na4JHdou8VdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "rpjezOmS8hmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText high ram required\n",
        "!curl -l https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz --output cc.en.300.bin.gz\n",
        "!gzip -d cc.en.300.bin.gz"
      ],
      "metadata": {
        "id": "yH-f3y75gpMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install datasets transformers tqdm\n",
        "!pip install nltk\n",
        "!pip install sklearn-crfsuite\n",
        "# !pip install fasttext\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "Vh1l-SaoYf_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification)\n",
        "from datasets import load_dataset, load_metric\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from transformers import TrainerCallback\n",
        "from sklearn_crfsuite import CRF\n",
        "from transformers import RobertaTokenizer, BertTokenizer, BertModel\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import numpy as np\n",
        "# import wandb\n",
        "import transformers\n",
        "from transformers import EarlyStoppingCallback\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models import Word2Vec, Doc2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "print(list(api.info()['models']))\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gensim.downloader as api\n",
        "# import fasttext\n",
        "# import fasttext.util\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn as  nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "-cVT88NR8NMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Pretrained Embedding Models"
      ],
      "metadata": {
        "id": "QdVPW8Z4hUrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "word2vec_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "metadata": {
        "id": "ARrnB8HPhXrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "import fasttext\n",
        "import fasttext.util"
      ],
      "metadata": {
        "id": "znjMrP2shbt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext_model =  fasttext.load_model('cc.en.300.bin')"
      ],
      "metadata": {
        "id": "gzbfm7ORhdpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "gg9ItPq98SWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")\n",
        "\n",
        "train_data = dataset[\"train\"]\n",
        "validation_data = dataset[\"validation\"]\n",
        "test_data = dataset[\"test\"]\n",
        "print(dataset)\n",
        "print('First example from the train split, ', train_data[0])\n",
        "print('Column names,', train_data.column_names)\n",
        "\n",
        "X_train = train_data['tokens']\n",
        "X_train_pos_tags = train_data['pos_tags']\n",
        "y_train = train_data['ner_tags']\n",
        "\n",
        "X_val = validation_data['tokens']\n",
        "X_val_pos_tags = validation_data['pos_tags']\n",
        "y_val = validation_data['ner_tags']\n",
        "\n",
        "X_test = test_data['tokens']\n",
        "X_test_pos_tags = test_data['pos_tags']\n",
        "y_test = test_data['ner_tags']"
      ],
      "metadata": {
        "id": "0zkBKAAK8P0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Constants"
      ],
      "metadata": {
        "id": "xFMX-Y68YwmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels_pad = 5\n",
        "lstm_units = 64\n",
        "embedding_dim = 300\n",
        "\n",
        "pos_embedding_dim = 50\n",
        "unique_pos_tags = set(tag for sequence in X_train_pos_tags+X_val_pos_tags+X_test_pos_tags for tag in sequence)\n",
        "pos_embeddings = np.random.rand(len(unique_pos_tags), pos_embedding_dim)\n",
        "pos_tag_to_index = {tag: idx for idx, tag in enumerate(unique_pos_tags)}\n",
        "print(\"unique pos tags, \", unique_pos_tags)\n",
        "print(\"unique pos tag to index, \", pos_tag_to_index)\n",
        "\n",
        "max_seq_length_train = max(len(seq) for seq in y_train)\n",
        "max_seq_length_val = max(len(seq) for seq in y_val)\n",
        "max_seq_length_test = max(len(seq) for seq in y_test)\n",
        "combined_max_seq_length = max(max_seq_length_train, max_seq_length_val, max_seq_length_test)\n",
        "\n",
        "label_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3, \"PAD\": 4}\n",
        "unique_labels = set(tag for sequence in y_train+y_val+y_test for tag in sequence)\n",
        "print(\"unique labels, \", unique_labels)"
      ],
      "metadata": {
        "id": "JWdjeIPCYyxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Functions"
      ],
      "metadata": {
        "id": "eMvQw_hYjg26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_labels(tag_sequence, pad_label=\"PAD\"):\n",
        "  convert_dict={0: 0, 1: 1, 3: 2, 4: 3, 'PAD': 4}\n",
        "  pad_value = convert_dict[pad_label]\n",
        "  encodings=[]\n",
        "  for tag in tag_sequence:\n",
        "    encodings.append(convert_dict.get(tag, pad_value))\n",
        "  return encodings\n",
        "\n",
        "def w2v_sequence_embedder_pad(token_list, max_sequence_length, embedding_dim=300):\n",
        "  embeddings = []\n",
        "\n",
        "  for token in token_list:\n",
        "    if token in word2vec_model:\n",
        "      embeddings.append(word2vec_model[token])\n",
        "    else:\n",
        "      embeddings.append(np.zeros(embedding_dim,))\n",
        "\n",
        "  num_padding = max_sequence_length - len(token_list)\n",
        "\n",
        "  if num_padding > 0:\n",
        "      embeddings.extend([np.zeros(embedding_dim,) for _ in range(num_padding)])\n",
        "  elif num_padding < 0:\n",
        "      embeddings = embeddings[:max_sequence_length]\n",
        "\n",
        "  sequence_embedding = np.array(embeddings)\n",
        "  return sequence_embedding\n",
        "\n",
        "\n",
        "def encode_sequences_pad(tag_sequence, max_sequence_length, pad_label=\"PAD\"):\n",
        "  pad_value = label_encoding[pad_label]\n",
        "\n",
        "  encodings=[]\n",
        "\n",
        "  for tag in tag_sequence:\n",
        "    encodings.append(label_encoding.get(tag, pad_value))\n",
        "\n",
        "  num_padding = max_sequence_length - len(tag_sequence)\n",
        "\n",
        "  if num_padding > 0:\n",
        "      encodings.extend([pad_value for _ in range(num_padding)])\n",
        "  elif num_padding < 0:\n",
        "      encodings = encodings[:max_sequence_length]\n",
        "  return encodings\n",
        "\n",
        "def encode_sequences(tag_sequence, pad_label='PAD'):\n",
        "  pad_value = label_encoding[pad_label]\n",
        "\n",
        "  encodings=[]\n",
        "\n",
        "  for tag in tag_sequence:\n",
        "    encodings.append(label_encoding.get(tag, pad_value))\n",
        "\n",
        "  return encodings\n",
        "\n",
        "def add_padding(tag_sequence, max_sequence_length, pad_label=\"PAD\"):\n",
        "  pad_value = label_encoding[pad_label]\n",
        "  encodings=[]\n",
        "\n",
        "  for tag in tag_sequence:\n",
        "      encodings.append(tag)\n",
        "\n",
        "  num_padding = max_sequence_length - len(tag_sequence)\n",
        "\n",
        "  if num_padding > 0:\n",
        "      encodings.extend([pad_value for _ in range(num_padding)])\n",
        "  elif num_padding < 0:\n",
        "      encodings = encodings[:max_sequence_length]\n",
        "  return encodings\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def print_classification_report(model, test_data, test_labels):\n",
        "  data_test = np.array(test_data)\n",
        "  labels_test = np.array(test_labels)\n",
        "  predictions = model.predict(data_test)\n",
        "  predictions = np.argmax(predictions, axis=-1)\n",
        "  true_labels = np.argmax(labels_test, axis=-1)\n",
        "\n",
        "  flat_predictions = predictions.flatten()\n",
        "  flat_true_labels = true_labels.flatten()\n",
        "\n",
        "  PAD_INDEX = label_encoding['PAD']\n",
        "  indices_to_keep = flat_true_labels != PAD_INDEX\n",
        "  filtered_predictions = flat_predictions[indices_to_keep]\n",
        "  filtered_true_labels = flat_true_labels[indices_to_keep]\n",
        "  accuracy = accuracy_score(filtered_true_labels, filtered_predictions)\n",
        "  print(f\"Accuracy: {accuracy}\")\n",
        "  report = classification_report(filtered_true_labels, filtered_predictions, target_names=[key for key in label_encoding if key != 'PAD'], labels=[label_encoding[key] for key in label_encoding if key != 'PAD'])\n",
        "  print(report)\n",
        "\n",
        "def plot_graphs(model_history, graph_name, graph_type, colour='blue'):\n",
        "  accuracy = model_history.history[graph_type]\n",
        "  epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(epochs, accuracy, colour, label=f'Training {graph_type}')\n",
        "  plt.title(graph_name)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel(graph_type)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def ft_word_embedder(token_sequence):\n",
        "    embeddings = []\n",
        "    for token in token_sequence:\n",
        "        embeddings.append(fasttext_model.get_word_vector(token))\n",
        "    return embeddings\n",
        "\n",
        "def w2v_word_embedder(token_list, embedding_dim=300):\n",
        "  embeddings = []\n",
        "\n",
        "  for token in token_list:\n",
        "    if token in word2vec_model:\n",
        "      embeddings.append(word2vec_model[token])\n",
        "    else:\n",
        "      embeddings.append(np.zeros(embedding_dim,))\n",
        "  return embeddings\n"
      ],
      "metadata": {
        "id": "W-5n_x_khj1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Analysis"
      ],
      "metadata": {
        "id": "Ox0qnyPp8bR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence Length Distribution"
      ],
      "metadata": {
        "id": "BFrAvcRnY-_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "bins = 30\n",
        "figsize = (10, 6)\n",
        "\n",
        "def plot_sentence_length_distribution(sentence_lengths, title, color):\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.hist(sentence_lengths, bins=bins, color=color, edgecolor='black', range=(0, 350))\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sentence Length')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.ylim(0, 300)\n",
        "    plt.show()\n",
        "\n",
        "train_sentence_lengths = [len(tokens) for tokens in X_train]\n",
        "plot_sentence_length_distribution(train_sentence_lengths, 'Training Set Sentence Length Distribution', 'skyblue')\n",
        "\n",
        "val_sentence_lengths = [len(tokens) for tokens in X_val]\n",
        "plot_sentence_length_distribution(val_sentence_lengths, 'Validation Set Sentence Length Distribution', 'magenta')\n",
        "\n",
        "test_sentence_lengths = [len(tokens) for tokens in X_test]\n",
        "plot_sentence_length_distribution(test_sentence_lengths, 'Test Set Sentence Length Distribution', 'orange')"
      ],
      "metadata": {
        "id": "9-sWfBq08gNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Distribution"
      ],
      "metadata": {
        "id": "Kc5UduIkZF5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ner_tags = [ner for sentence in train_data['ner_tags'] for ner in sentence]\n",
        "ner_tag_counts = Counter(ner_tags)\n",
        "\n",
        "ner_labels, ner_counts = zip(*ner_tag_counts.items())\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(ner_labels, ner_counts, color='orange', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('NER Tag Distribution')\n",
        "plt.xlabel('NER Tags')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Js3cYrPGZMgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS Tag Distribution"
      ],
      "metadata": {
        "id": "4SfZKE5hZSGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pos_tags = [pos for sentence in train_data['pos_tags'] for pos in sentence]\n",
        "pos_tag_counts = Counter(pos_tags)\n",
        "\n",
        "pos_labels, pos_counts = zip(*pos_tag_counts.items())\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(pos_labels, pos_counts, color='skyblue', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('POS Tag Distribution')\n",
        "plt.xlabel('POS Tags')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NiAAcRY6ZVL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Acronyms and Long Forms in the Data"
      ],
      "metadata": {
        "id": "6DqLdes4ZZxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_specific_words(data, tags_of_interest):\n",
        "    words_of_interest = {tag: [] for tag in tags_of_interest}\n",
        "\n",
        "    for tokens, ner_tags in zip(data['tokens'], data['ner_tags']):\n",
        "        for token, tag in zip(tokens, ner_tags):\n",
        "            if tag in tags_of_interest:\n",
        "                words_of_interest[tag].append(token)\n",
        "\n",
        "    return words_of_interest\n",
        "\n",
        "interest_tags = ['B-AC', 'B-LF', 'I-LF']\n",
        "\n",
        "train_words = extract_specific_words(train_data, interest_tags)\n",
        "validation_words = extract_specific_words(validation_data, interest_tags)\n",
        "test_words = extract_specific_words(test_data, interest_tags)\n",
        "\n",
        "print(\"Words tagged as 'B-AC' in Training Data:\", train_words['B-AC'])\n",
        "print(\"Words tagged as 'B-LF' in Training Data:\", train_words['B-LF'])\n",
        "print(\"Words tagged as 'I-LF' in Training Data:\", train_words['I-LF'])\n",
        "\n",
        "\n",
        "print(\"Words tagged as 'B-AC' in Validation Data:\", validation_words['B-AC'])\n",
        "print(\"Words tagged as 'B-LF' in Validation Data:\", validation_words['B-LF'])\n",
        "print(\"Words tagged as 'I-LF' in Validation Data:\", validation_words['I-LF'])\n",
        "\n",
        "print(\"Words tagged as 'B-AC' in Test Data:\", test_words['B-AC'])\n",
        "print(\"Words tagged as 'B-LF' in Test Data:\", test_words['B-LF'])\n",
        "print(\"Words tagged as 'I-LF' in Test Data:\", test_words['I-LF'])"
      ],
      "metadata": {
        "id": "4Wi3ToRnZfAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Co-occurance Entity Types"
      ],
      "metadata": {
        "id": "7GZkJFkWZntg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "co_occurrences = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for sentence_tags in train_data['ner_tags']:\n",
        "    unique_tags = set(sentence_tags)\n",
        "    for tag1 in unique_tags:\n",
        "        for tag2 in unique_tags:\n",
        "            if tag1 != tag2:\n",
        "                co_occurrences[tag1][tag2] += 1\n",
        "                co_occurrences[tag2][tag1] += 1\n",
        "\n",
        "co_occurrence_matrix = pd.DataFrame(co_occurrences).fillna(0)\n",
        "co_occurrence_matrix = co_occurrence_matrix.astype(int)\n",
        "\n",
        "print(co_occurrence_matrix)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(co_occurrence_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
        "plt.title(\"BIO Tag Type Co-occurrence Matrix\")\n",
        "plt.xlabel(\"BIO Tag Types\")\n",
        "plt.ylabel(\"BIO Tag Types\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A0BFox7bZu8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1: Feature Representation Experimentation"
      ],
      "metadata": {
        "id": "kFEJDs-vaERT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Experiment: FastText"
      ],
      "metadata": {
        "id": "awiFCOiZaP97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare labels for SVM"
      ],
      "metadata": {
        "id": "UdV9sjNSauvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flat_labels_train = [item for sublist in y_train for item in sublist]\n",
        "flat_labels_val = [item for sublist in y_val for item in sublist]\n",
        "flat_labels_test = [item for sublist in y_test for item in sublist]\n",
        "\n",
        "\n",
        "encoded_labels_train = []\n",
        "for sample in flat_labels_train:\n",
        "    encoded_labels_train.append(label_encoding.get(sample))\n",
        "\n",
        "encoded_labels_val = []\n",
        "for sample in flat_labels_val:\n",
        "    encoded_labels_val.append(label_encoding.get(sample))\n",
        "\n",
        "encoded_labels_test = []\n",
        "for sample in flat_labels_test:\n",
        "    encoded_labels_test.append(label_encoding.get(sample))"
      ],
      "metadata": {
        "id": "yMv9kUJNaNYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare embeddings for SVM"
      ],
      "metadata": {
        "id": "MzLqXyjrapor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flat_tokens_train = [item for sublist in X_train for item in sublist]\n",
        "flat_tokens_val = [item for sublist in X_val for item in sublist]\n",
        "flat_tokens_test = [item for sublist in X_test for item in sublist]\n",
        "\n",
        "train_token_ft_embeddings = ft_word_embedder(flat_tokens_train)\n",
        "val_token_ft_embeddings = ft_word_embedder(flat_tokens_val)\n",
        "test_token_ft_embeddings = ft_word_embedder(flat_tokens_test)"
      ],
      "metadata": {
        "id": "2ZMPwbUWaotw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train SVM"
      ],
      "metadata": {
        "id": "92H0TveGay6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "svm_model_ft = svm.LinearSVC()\n",
        "svm_model_ft.fit(train_token_ft_embeddings, encoded_labels_train)"
      ],
      "metadata": {
        "id": "IO2ensb9a2vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict and obtain metrics"
      ],
      "metadata": {
        "id": "acNIZYpNa6FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Evaluation on validation set\n",
        "svm_ft_prediction_val = svm_model_ft.predict(val_token_ft_embeddings)\n",
        "accuracy = accuracy_score(encoded_labels_val, svm_ft_prediction_val)\n",
        "cm = confusion_matrix(encoded_labels_val, svm_ft_prediction_val)\n",
        "print(\"Validation report:\\n\", classification_report(encoded_labels_val, svm_ft_prediction_val))\n",
        "print(\"Validation accuracy:\", accuracy)\n",
        "print(\"Validation confusion matrix:\\n\", cm)\n",
        "\n",
        "# Evaluation on test set\n",
        "svm_ft_prediction_test = svm_model_ft.predict(test_token_ft_embeddings)\n",
        "accuracy = accuracy_score(encoded_labels_test, svm_ft_prediction_test)\n",
        "cm = confusion_matrix(encoded_labels_test, svm_ft_prediction_test)\n",
        "print(\"Test report:\\n\", classification_report(encoded_labels_test, svm_ft_prediction_test))\n",
        "print(\"Test accuracy:\", accuracy)\n",
        "print(\"test confusion matrix:\\n\", cm)"
      ],
      "metadata": {
        "id": "0F2fbOqoa8Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Experiment: Bag of Words (BoW)"
      ],
      "metadata": {
        "id": "lULReFH9a_mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "train_token_bow_embeddings = vectorizer.fit_transform(flat_tokens_train)\n",
        "val_token_bow_embeddings = vectorizer.transform(flat_tokens_val)\n",
        "test_token_bow_embeddings = vectorizer.transform(flat_tokens_test)\n",
        "\n",
        "svm_model_bow = svm.LinearSVC()\n",
        "svm_model_bow.fit(train_token_bow_embeddings, encoded_labels_train)"
      ],
      "metadata": {
        "id": "GY7PiBM7bEef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on validation set\n",
        "svm_bow_prediction_val = svm_model_bow.predict(val_token_bow_embeddings)\n",
        "accuracy = accuracy_score(encoded_labels_val, svm_bow_prediction_val)\n",
        "cm = confusion_matrix(encoded_labels_val, svm_bow_prediction_val)\n",
        "print(\"Validation report:\\n\", classification_report(encoded_labels_val, svm_bow_prediction_val))\n",
        "print(\"Validation accuracy:\", accuracy)\n",
        "print(\"Validation confusion matrix:\\n\", cm)\n",
        "\n",
        "# Evaluate on test set\n",
        "svm_bow_prediction_test = svm_model_bow.predict(test_token_bow_embeddings)\n",
        "accuracy = accuracy_score(encoded_labels_test, svm_bow_prediction_test)\n",
        "cm = confusion_matrix(encoded_labels_test, svm_bow_prediction_test)\n",
        "print(\"Test report:\\n\", classification_report(encoded_labels_test, svm_bow_prediction_test))\n",
        "print(\"Test accuracy:\", accuracy)\n",
        "print(\"test confusion matrix:\\n\", cm)"
      ],
      "metadata": {
        "id": "V5K1575JbGSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Experiment: Word2Vec"
      ],
      "metadata": {
        "id": "6NWjVLEibKM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "train_token_w2v_embeddings = w2v_word_embedder(flat_tokens_train)\n",
        "val_token_w2v_embeddings = w2v_word_embedder(flat_tokens_val)\n",
        "test_token_w2v_embeddings = w2v_word_embedder(flat_tokens_test)\n",
        "\n",
        "svm_model_w2v = svm.LinearSVC()\n",
        "svm_model_w2v.fit(train_token_w2v_embeddings, encoded_labels_train)"
      ],
      "metadata": {
        "id": "6m2FogOIbNzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on validation set\n",
        "svm_w2v_prediction_val = svm_model_w2v.predict(val_token_w2v_embeddings)\n",
        "accuracy = accuracy_score(encoded_labels_val, svm_w2v_prediction_val)\n",
        "cm = confusion_matrix(encoded_labels_val, svm_w2v_prediction_val)\n",
        "print(\"Validation report:\\n\", classification_report(encoded_labels_val, svm_w2v_prediction_val))\n",
        "print(\"Validation accuracy:\", accuracy)\n",
        "print(\"Validation confusion matrix:\\n\", cm)\n",
        "\n",
        "# Evaluate on test set\n",
        "svm_w2v_prediction_test = svm_model_w2v.predict(test_token_w2v_embeddings)\n",
        "accuracy = accuracy_score(encoded_labels_test, svm_w2v_prediction_test)\n",
        "cm = confusion_matrix(encoded_labels_test, svm_w2v_prediction_test)\n",
        "print(\"Test report:\\n\", classification_report(encoded_labels_test, svm_w2v_prediction_test))\n",
        "print(\"Test accuracy:\", accuracy)\n",
        "print(\"test confusion matrix:\\n\", cm)"
      ],
      "metadata": {
        "id": "RV0KI6EzbQa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 2: Model Experimentation"
      ],
      "metadata": {
        "id": "oUOf4MVSWCHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Experiment: CRF"
      ],
      "metadata": {
        "id": "3HDoaB5fXrlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word2features(sentence:list, i:int):\n",
        "    word = sentence[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_first': i == 0, #if the word is a first word\n",
        "        'is_last': i == len(sentence) - 1,  #if the word is a last word\n",
        "        'is_capitalized': word[0].upper() == word[0],\n",
        "        'is_all_caps': word.upper() == word,      #word is in uppercase\n",
        "        'is_all_lower': word.lower() == word,      #word is in lowercase\n",
        "         #prefix of the word\n",
        "        'prefix-1': word[0],\n",
        "        'prefix-2': word[:2],\n",
        "        'prefix-3': word[:3],\n",
        "         #suffix of the word\n",
        "        'suffix-1': word[-1],\n",
        "        'suffix-2': word[-2:],\n",
        "        'suffix-3': word[-3:],\n",
        "         #extracting previous word\n",
        "        'prev_word': '' if i == 0 else sentence[i-1][0],\n",
        "         #extracting next word\n",
        "        'next_word': '' if i == len(sentence)-1 else sentence[i+1][0],\n",
        "        'has_hyphen': '-' in word,    #if word has hypen\n",
        "        'is_numeric': word.isdigit(),  #if word is in numeric\n",
        "        'capitals_inside': word[1:].lower() != word[1:]\n",
        "    }\n",
        "    return features\n",
        "\n",
        "def sent2features(tokens:list):\n",
        "    return [word2features(tokens, index) for index in range(len(tokens))]\n",
        "\n",
        "train_token_crf_embeddings=[sent2features(tokens) for tokens in X_train]\n",
        "val_token_crf_embeddings=[sent2features(tokens) for tokens in X_val]\n",
        "test_token_crf_embeddings=[sent2features(tokens) for tokens in X_test]\n",
        "\n",
        "crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "\n",
        "# Train crf model\n",
        "try:\n",
        "    crf.fit(train_token_crf_embeddings, y_train)\n",
        "except AttributeError:\n",
        "    pass\n",
        "\n",
        "print(train_token_crf_embeddings[0])\n",
        "print(len(y_train[0]))\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Evaluate on val set\n",
        "crf_prediction_val = crf.predict(val_token_crf_embeddings)\n",
        "crf_prediction_val_flat = [label for sublist in crf_prediction_val for label in sublist] # Flatten nested lists for evaluation\n",
        "accuracy = accuracy_score(flat_labels_val, crf_prediction_val_flat)\n",
        "cm = confusion_matrix(flat_labels_val, crf_prediction_val_flat)\n",
        "print(\"Validation report:\\n\", classification_report(flat_labels_val, crf_prediction_val_flat))\n",
        "print(\"Validation accuracy:\", accuracy)\n",
        "print(\"Validation confusion matrix:\\n\", cm)\n",
        "\n",
        "# Evaluate on test set\n",
        "crf_prediction_test = crf.predict(test_token_crf_embeddings)\n",
        "crf_prediction_test_flat = [label for sublist in crf_prediction_test for label in sublist] # Flatten nested lists for evaluation\n",
        "accuracy = accuracy_score(flat_labels_test, crf_prediction_test_flat)\n",
        "cm = confusion_matrix(flat_labels_test, crf_prediction_test_flat)\n",
        "print(\"Test report:\\n\", classification_report(flat_labels_test, crf_prediction_test_flat))\n",
        "print(\"Test accuracy:\", accuracy)\n",
        "print(\"Test confusion matrix:\\n\", cm)"
      ],
      "metadata": {
        "id": "oZKDiB7oX7E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Experiment: LSTM"
      ],
      "metadata": {
        "id": "jsaM5Ng1WasQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Masking, InputLayer, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import gensim.downloader as api\n",
        "import os\n",
        "\n",
        "# Ensure the directory for saving graphs exists\n",
        "os.makedirs('lstmgraphs', exist_ok=True)\n",
        "\n",
        "# Load the Word2Vec model\n",
        "print(\"Loading Word2Vec model...\")\n",
        "word2vec_model = api.load('word2vec-google-news-300')\n",
        "print(\"Word2Vec model loaded.\")\n",
        "\n",
        "# Load the dataset from Hugging Face's 'datasets'\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")\n",
        "train_data = dataset[\"train\"]\n",
        "validation_data = dataset[\"validation\"]\n",
        "test_data = dataset[\"test\"]\n",
        "print(\"Dataset loaded.\")\n",
        "\n",
        "# Constants\n",
        "MAX_SEQ_LENGTH = 323\n",
        "NUM_LABELS = 5\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "label_encoding = {\"B-O\": 1, \"B-AC\": 2, \"B-LF\": 3, \"I-LF\": 4, \"PAD\": 0}\n",
        "\n",
        "# Function to encode labels\n",
        "def encode_labels(labels):\n",
        "    print(\"Encoding labels...\")\n",
        "    encoded = [[label_encoding[label] for label in sequence] for sequence in labels]\n",
        "    print(\"Labels encoded.\")\n",
        "    return encoded\n",
        "\n",
        "# Function to embed tokens using Word2Vec\n",
        "def embed_tokens(tokens):\n",
        "    print(\"Embedding tokens...\")\n",
        "    embedded_tokens = []\n",
        "    for token_sequence in tokens:\n",
        "        sequence_embedding = [word2vec_model[word] if word in word2vec_model else np.zeros(EMBEDDING_DIM) for word in token_sequence]\n",
        "        if len(sequence_embedding) < MAX_SEQ_LENGTH:\n",
        "            sequence_embedding += [np.zeros(EMBEDDING_DIM) for _ in range(MAX_SEQ_LENGTH - len(sequence_embedding))]\n",
        "        embedded_tokens.append(np.array(sequence_embedding[:MAX_SEQ_LENGTH]))\n",
        "    print(\"Tokens embedded.\")\n",
        "    return np.array(embedded_tokens)\n",
        "\n",
        "def pad_sequences_custom(data, maxlen, dtype='float32'):\n",
        "    print(f\"Padding sequences to a maximum length of {maxlen}...\")\n",
        "    padded_data = tf.keras.preprocessing.sequence.pad_sequences(data, maxlen=maxlen, dtype=dtype, padding='post', truncating='post', value=0.0)\n",
        "    print(\"Sequences padded.\")\n",
        "    return padded_data\n",
        "\n",
        "# Prepare datasets\n",
        "def prepare_data(data):\n",
        "    print(\"Preparing data...\")\n",
        "    tokens = data['tokens']\n",
        "    labels = encode_labels(data['ner_tags'])\n",
        "    embedded_tokens = embed_tokens(tokens)\n",
        "    print(\"Token Embeddings: \", embedded_tokens.shape)\n",
        "    padded_labels = pad_sequences_custom(labels, MAX_SEQ_LENGTH, dtype='int32')\n",
        "    print(\"Data prepared.\")\n",
        "    return embedded_tokens, to_categorical(padded_labels, num_classes=NUM_LABELS), padded_labels\n",
        "\n",
        "\n",
        "print(\"Preparing training data...\")\n",
        "X_train, y_train, train_padded_labels = prepare_data(train_data)\n",
        "sample_weight = np.where(train_padded_labels == label_encoding[\"PAD\"], 0, 1)\n",
        "print(\"Training data ready.\")\n",
        "\n",
        "print(\"Preparing validation data...\")\n",
        "X_val, y_val, val_padded_labels = prepare_data(validation_data)\n",
        "val_sample_weight = np.where(val_padded_labels == label_encoding[\"PAD\"], 0, 1)\n",
        "print(\"Validation data ready.\")\n",
        "\n",
        "print(\"Preparing test data...\")\n",
        "X_test, y_test, test_padded_labels = prepare_data(test_data)\n",
        "test_sample_weight = np.where(test_padded_labels == label_encoding[\"PAD\"], 0, 1)\n",
        "print(\"Test data ready.\")\n",
        "\n",
        "def create_lstm_model(num_labels, lstm_units, vocab_size, embedding_size, w2v_weights, max_sequence_length):\n",
        "    model = Sequential()\n",
        "    model.add(Masking(mask_value=0.0, input_shape=(max_sequence_length, embedding_size)))\n",
        "    model.add(LSTM(lstm_units, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(num_labels, activation='softmax')))\n",
        "    return model\n",
        "\n",
        "NUM_LABELS_PAD = 5\n",
        "LSTM_UNITS = 64\n",
        "COMBINED_MAX_SEQ_LENGTH = 323\n",
        "w2v_weights = word2vec_model.vectors\n",
        "\n",
        "vocab_size, embedding_size = w2v_weights.shape\n",
        "print(\"Word2Vec weights shape:\", w2v_weights.shape)\n",
        "print(\"Creating LSTM model...\")\n",
        "lstm_model = create_lstm_model(NUM_LABELS_PAD, LSTM_UNITS, vocab_size, EMBEDDING_DIM, w2v_weights, COMBINED_MAX_SEQ_LENGTH)\n",
        "print(\"LSTM model created.\")\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(\"Before i am pased in: \", X_train.shape)\n",
        "print(\"Starting training...\")\n",
        "history = lstm_model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    sample_weight=sample_weight\n",
        ")\n",
        "print(\"Training completed.\")\n",
        "\n",
        "def evaluate_model(model, X, y):\n",
        "    print(\"Evaluating model...\")\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "    y_true = np.argmax(y, axis=-1)\n",
        "\n",
        "    mask = y_true != label_encoding[\"PAD\"]\n",
        "    y_pred_masked = y_pred[mask]\n",
        "    y_true_masked = y_true[mask]\n",
        "\n",
        "    accuracy = np.mean(y_pred_masked == y_true_masked)\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    labels_without_pad = [label for label in label_encoding if label != \"PAD\"]\n",
        "    label_values_without_pad = [label_encoding[label] for label in labels_without_pad]\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true_masked, y_pred_masked, labels=label_values_without_pad, target_names=labels_without_pad, zero_division=1))\n",
        "\n",
        "    cm = confusion_matrix(y_true_masked, y_pred_masked, labels=label_values_without_pad)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='seismic', xticklabels=labels_without_pad, yticklabels=labels_without_pad)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.savefig('lstmgraphs/confusion_matrix.png')\n",
        "    plt.show()\n",
        "\n",
        "# Run evaluation\n",
        "print(\"Evaluating on test data...\")\n",
        "evaluate_model(lstm_model, X_test, y_test)\n",
        "print(\"Evaluation completed.\")"
      ],
      "metadata": {
        "id": "1xeVhMZmWBdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Experiment: BiLSTM"
      ],
      "metadata": {
        "id": "KO2ZmB6BW3D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code: # Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Masking, InputLayer, Embedding, Bidirectional\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import gensim.downloader as api\n",
        "import os\n",
        "\n",
        "os.makedirs('lstmgraphs', exist_ok=True)\n",
        "\n",
        "print(\"Loading Word2Vec model...\")\n",
        "word2vec_model = api.load('word2vec-google-news-300')\n",
        "print(\"Word2Vec model loaded.\")\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")\n",
        "train_data = dataset[\"train\"]\n",
        "validation_data = dataset[\"validation\"]\n",
        "test_data = dataset[\"test\"]\n",
        "print(\"Dataset loaded.\")\n",
        "\n",
        "MAX_SEQ_LENGTH = 323\n",
        "NUM_LABELS = 5\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "label_encoding = {\"B-O\": 1, \"B-AC\": 2, \"B-LF\": 3, \"I-LF\": 4, \"PAD\": 0}\n",
        "\n",
        "def encode_labels(labels):\n",
        "    print(\"Encoding labels...\")\n",
        "    encoded = [[label_encoding[label] for label in sequence] for sequence in labels]\n",
        "    print(\"Labels encoded.\")\n",
        "    return encoded\n",
        "\n",
        "def embed_tokens(tokens):\n",
        "    print(\"Embedding tokens...\")\n",
        "    embedded_tokens = []\n",
        "    for token_sequence in tokens:\n",
        "        sequence_embedding = [word2vec_model[word] if word in word2vec_model else np.zeros(EMBEDDING_DIM) for word in token_sequence]\n",
        "        if len(sequence_embedding) < MAX_SEQ_LENGTH:\n",
        "            sequence_embedding += [np.zeros(EMBEDDING_DIM) for _ in range(MAX_SEQ_LENGTH - len(sequence_embedding))]\n",
        "        embedded_tokens.append(np.array(sequence_embedding[:MAX_SEQ_LENGTH]))\n",
        "    print(\"Tokens embedded.\")\n",
        "    return np.array(embedded_tokens)\n",
        "\n",
        "def pad_sequences_custom(data, maxlen, dtype='float32'):\n",
        "    print(f\"Padding sequences to a maximum length of {maxlen}...\")\n",
        "    padded_data = tf.keras.preprocessing.sequence.pad_sequences(data, maxlen=maxlen, dtype=dtype, padding='post', truncating='post', value=0.0)\n",
        "    print(\"Sequences padded.\")\n",
        "    return padded_data\n",
        "\n",
        "def prepare_data(data):\n",
        "    print(\"Preparing data...\")\n",
        "    tokens = data['tokens']\n",
        "    labels = encode_labels(data['ner_tags'])\n",
        "    embedded_tokens = embed_tokens(tokens)\n",
        "    print(\"Token Embeddings: \", embedded_tokens.shape)\n",
        "    padded_labels = pad_sequences_custom(labels, MAX_SEQ_LENGTH, dtype='int32')\n",
        "    print(\"Data prepared.\")\n",
        "    return embedded_tokens, to_categorical(padded_labels, num_classes=NUM_LABELS), padded_labels\n",
        "\n",
        "\n",
        "print(\"Preparing training data...\")\n",
        "X_train, y_train, train_padded_labels = prepare_data(train_data)\n",
        "sample_weight = np.where(train_padded_labels == label_encoding[\"PAD\"], 0, 1)\n",
        "print(\"Training data ready.\")\n",
        "\n",
        "print(\"Preparing validation data...\")\n",
        "X_val, y_val, val_padded_labels = prepare_data(validation_data)\n",
        "val_sample_weight = np.where(val_padded_labels == label_encoding[\"PAD\"], 0, 1)\n",
        "print(\"Validation data ready.\")\n",
        "\n",
        "print(\"Preparing test data...\")\n",
        "X_test, y_test, test_padded_labels = prepare_data(test_data)\n",
        "test_sample_weight = np.where(test_padded_labels == label_encoding[\"PAD\"], 0, 1)\n",
        "print(\"Test data ready.\")\n",
        "\n",
        "def create_bilstm_model(num_labels, lstm_units, vocab_size, embedding_size, w2v_weights, max_sequence_length):\n",
        "    model = Sequential()\n",
        "    model.add(Masking(mask_value=0.0, input_shape=(max_sequence_length, embedding_size)))\n",
        "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(num_labels, activation='softmax')))\n",
        "    return model\n",
        "\n",
        "NUM_LABELS_PAD = 5\n",
        "LSTM_UNITS = 64\n",
        "COMBINED_MAX_SEQ_LENGTH = 323\n",
        "w2v_weights = word2vec_model.vectors\n",
        "\n",
        "vocab_size, embedding_size = w2v_weights.shape\n",
        "print(\"Word2Vec weights shape:\", w2v_weights.shape)\n",
        "print(\"Creating LSTM model...\")\n",
        "lstm_model = create_bilstm_model(NUM_LABELS_PAD, LSTM_UNITS, vocab_size, EMBEDDING_DIM, w2v_weights, COMBINED_MAX_SEQ_LENGTH)\n",
        "print(\"LSTM model created.\")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Compile the modelfjl\n",
        "print(\"Before i am pased in: \", X_train.shape)\n",
        "print(\"Starting training...\")\n",
        "history = lstm_model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    sample_weight=sample_weight\n",
        ")\n",
        "print(\"Training completed.\")\n",
        "\n",
        "def evaluate_model(model, X, y):\n",
        "    print(\"Evaluating model...\")\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "    y_true = np.argmax(y, axis=-1)\n",
        "\n",
        "    mask = y_true != label_encoding[\"PAD\"]\n",
        "    y_pred_masked = y_pred[mask]\n",
        "    y_true_masked = y_true[mask]\n",
        "\n",
        "    accuracy = np.mean(y_pred_masked == y_true_masked)\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    labels_without_pad = [label for label in label_encoding if label != \"PAD\"]\n",
        "    label_values_without_pad = [label_encoding[label] for label in labels_without_pad]\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true_masked, y_pred_masked, labels=label_values_without_pad, target_names=labels_without_pad, zero_division=1))\n",
        "\n",
        "    cm = confusion_matrix(y_true_masked, y_pred_masked, labels=label_values_without_pad)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='seismic', xticklabels=labels_without_pad, yticklabels=labels_without_pad)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.savefig('lstmgraphs/confusion_matrix.png')  # Save confusion matrix to a file\n",
        "    plt.show()\n",
        "\n",
        "print(\"Evaluating on test data...\")\n",
        "evaluate_model(lstm_model, X_test, y_test)\n",
        "print(\"Evaluation completed.\")"
      ],
      "metadata": {
        "id": "W5cU-NtSXElA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3: POS Tags Inclusion and Additional Filtered Dataset"
      ],
      "metadata": {
        "id": "G5NooaLqc6LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Experiment: Add POS Tags"
      ],
      "metadata": {
        "id": "3xwfJE97dtjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Masking, InputLayer, Embedding, Bidirectional\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import gensim.downloader as api\n",
        "import os\n",
        "\n",
        "os.makedirs('lstmgraphs', exist_ok=True)\n",
        "\n",
        "print(\"Loading Word2Vec model...\")\n",
        "word2vec_model = api.load('word2vec-google-news-300')\n",
        "print(\"Word2Vec model loaded.\")\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")\n",
        "train_data = dataset[\"train\"]\n",
        "validation_data = dataset[\"validation\"]\n",
        "test_data = dataset[\"test\"]\n",
        "print(\"Dataset loaded.\")\n",
        "print(\"Preparing POS embeddings...\")\n",
        "train_pos_tags = train_data['pos_tags']\n",
        "unique_pos_tags = set(tag for sequence in train_pos_tags for tag in sequence)\n",
        "pos_embedding_dim = 50\n",
        "pos_embeddings = np.random.rand(len(unique_pos_tags), pos_embedding_dim)\n",
        "pos_tag_to_index = {tag: idx for idx, tag in enumerate(unique_pos_tags)}\n",
        "print(\"POS embeddings prepared.\")\n",
        "\n",
        "MAX_SEQ_LENGTH = 323\n",
        "NUM_LABELS = 5\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "label_encoding = {\"B-O\": 1, \"B-AC\": 2, \"B-LF\": 3, \"I-LF\": 4, \"PAD\": 0}\n",
        "\n",
        "def encode_labels(labels):\n",
        "    print(\"Encoding labels...\")\n",
        "    encoded = [[label_encoding[label] for label in sequence] for sequence in labels]\n",
        "    print(\"Labels encoded.\")\n",
        "    return encoded\n",
        "\n",
        "def embed_tokens(tokens, pos_tags, pos_embeddings, pos_tag_to_index, pos_embedding_dim=50):\n",
        "    print(\"Embedding tokens with POS...\")\n",
        "    embedded_sequences = []\n",
        "    for token_sequence, pos_sequence in zip(tokens, pos_tags):\n",
        "        sequence_embeddings = []\n",
        "        for token, pos_tag in zip(token_sequence, pos_sequence):\n",
        "            word_embedding = word2vec_model[token] if token in word2vec_model else np.zeros(EMBEDDING_DIM)\n",
        "            pos_embedding = get_pos_embedding(pos_tag, pos_embeddings, pos_tag_to_index, pos_embedding_dim)\n",
        "            combined_embedding = np.concatenate((word_embedding, pos_embedding))\n",
        "            sequence_embeddings.append(combined_embedding)\n",
        "\n",
        "        if len(sequence_embeddings) < MAX_SEQ_LENGTH:\n",
        "            padding = np.zeros(EMBEDDING_DIM + pos_embedding_dim,)\n",
        "            sequence_embeddings.extend([padding for _ in range(MAX_SEQ_LENGTH - len(sequence_embeddings))])\n",
        "\n",
        "        sequence_embeddings = sequence_embeddings[:MAX_SEQ_LENGTH]\n",
        "        embedded_sequences.append(np.array(sequence_embeddings))\n",
        "\n",
        "    print(\"Tokens with POS embedded.\")\n",
        "    return np.array(embedded_sequences)\n",
        "\n",
        "def get_pos_embedding(pos_tag, pos_embeddings, pos_tag_to_index, pos_embedding_dim=50):\n",
        "    if pos_tag not in pos_tag_to_index:\n",
        "        return np.zeros(pos_embedding_dim)\n",
        "    index = pos_tag_to_index[pos_tag]\n",
        "    return pos_embeddings[index]\n",
        "\n",
        "def pad_sequences_custom(data, maxlen, dtype='float32'):\n",
        "    print(f\"Padding sequences to a maximum length of {maxlen}...\")\n",
        "    padded_data = tf.keras.preprocessing.sequence.pad_sequences(data, maxlen=maxlen, dtype=dtype, padding='post', truncating='post', value=0.0)\n",
        "    print(\"Sequences padded.\")\n",
        "    return padded_data\n",
        "\n",
        "def prepare_data(data):\n",
        "    print(\"Preparing data...\")\n",
        "    tokens = data['tokens']\n",
        "    pos_tags = data['pos_tags']\n",
        "    labels = encode_labels(data['ner_tags'])\n",
        "    embedded_tokens = embed_tokens(tokens,pos_tags,pos_embeddings, pos_tag_to_index, pos_embedding_dim=50)\n",
        "    print(\"Token Embeddings: \", embedded_tokens.shape)\n",
        "    padded_labels = pad_sequences_custom(labels, MAX_SEQ_LENGTH, dtype='int32')\n",
        "    print(\"Data prepared.\")\n",
        "    return embedded_tokens, to_categorical(padded_labels, num_classes=NUM_LABELS), padded_labels\n",
        "\n",
        "\n",
        "print(\"Preparing training data...\")\n",
        "X_train, y_train, train_padded_labels = prepare_data(train_data)\n",
        "sample_weight = np.where(train_padded_labels == label_encoding[\"PAD\"], 0, 1)\n",
        "print(\"Training data ready.\")\n",
        "\n",
        "print(\"Preparing validation data...\")\n",
        "X_val, y_val, val_padded_labels = prepare_data(validation_data)\n",
        "val_sample_weight = np.where(val_padded_labels == label_encoding[\"PAD\"], 0, 1)\n",
        "print(\"Validation data ready.\")\n",
        "\n",
        "print(\"Preparing test data...\")\n",
        "X_test, y_test, test_padded_labels = prepare_data(test_data)\n",
        "test_sample_weight = np.where(test_padded_labels == label_encoding[\"PAD\"], 0, 1)\n",
        "print(\"Test data ready.\")\n",
        "\n",
        "def create_bilstm_model(num_labels, lstm_units, vocab_size, embedding_size, w2v_weights, max_sequence_length):\n",
        "    model = Sequential()\n",
        "    model.add(Masking(mask_value=0.0, input_shape=(max_sequence_length, embedding_size+50)))\n",
        "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(num_labels, activation='softmax')))\n",
        "    return model\n",
        "\n",
        "NUM_LABELS_PAD = 5\n",
        "LSTM_UNITS = 64\n",
        "COMBINED_MAX_SEQ_LENGTH = 323\n",
        "w2v_weights = word2vec_model.vectors\n",
        "\n",
        "vocab_size, embedding_size = w2v_weights.shape\n",
        "print(\"Word2Vec weights shape:\", w2v_weights.shape)\n",
        "print(\"Creating LSTM model...\")\n",
        "lstm_model = create_bilstm_model(NUM_LABELS_PAD, LSTM_UNITS, vocab_size, EMBEDDING_DIM, w2v_weights, COMBINED_MAX_SEQ_LENGTH)\n",
        "print(\"LSTM model created.\")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Compile the modelfjl\n",
        "print(\"Before i am pased in: \", X_train.shape)\n",
        "print(\"Starting training...\")\n",
        "history = lstm_model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    sample_weight=sample_weight\n",
        ")\n",
        "print(\"Training completed.\")\n",
        "\n",
        "def evaluate_model(model, X, y):\n",
        "    print(\"Evaluating model...\")\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "    y_true = np.argmax(y, axis=-1)\n",
        "\n",
        "    mask = y_true != label_encoding[\"PAD\"]\n",
        "    y_pred_masked = y_pred[mask]\n",
        "    y_true_masked = y_true[mask]\n",
        "\n",
        "    accuracy = np.mean(y_pred_masked == y_true_masked)\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    labels_without_pad = [label for label in label_encoding if label != \"PAD\"]\n",
        "    label_values_without_pad = [label_encoding[label] for label in labels_without_pad]\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true_masked, y_pred_masked, labels=label_values_without_pad, target_names=labels_without_pad, zero_division=1))\n",
        "\n",
        "    cm = confusion_matrix(y_true_masked, y_pred_masked, labels=label_values_without_pad)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='seismic', xticklabels=labels_without_pad, yticklabels=labels_without_pad)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.savefig('lstmgraphs/confusion_matrix.png')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Evaluating on test data...\")\n",
        "evaluate_model(lstm_model, X_test, y_test)\n",
        "print(\"Evaluation completed.\")"
      ],
      "metadata": {
        "id": "DARB7g2xdBUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Experiment: Additional Data"
      ],
      "metadata": {
        "id": "t_QNMKzodzMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Masking, InputLayer, Embedding, Bidirectional\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import gensim.downloader as api\n",
        "import os\n",
        "\n",
        "os.makedirs('lstmgraphs', exist_ok=True)\n",
        "\n",
        "print(\"Loading Word2Vec model...\")\n",
        "word2vec_model = api.load('word2vec-google-news-300')\n",
        "print(\"Word2Vec model loaded.\")\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"surrey-nlp/PLOD-CW\")\n",
        "train_data = dataset[\"train\"]\n",
        "validation_data = dataset[\"validation\"]\n",
        "test_data = dataset[\"test\"]\n",
        "print(\"Dataset loaded.\")\n",
        "\n",
        "print(\"Loading filtered dataset...\")\n",
        "filtereddataset = load_dataset(\"surrey-nlp/PLOD-filtered\")\n",
        "filtered_train_data = filtereddataset[\"train\"]\n",
        "filtered_valid_data = filtereddataset[\"validation\"]\n",
        "filtered_test_data = filtereddataset[\"test\"]\n",
        "print(\"Dataset loaded.\")\n",
        "\n",
        "MAX_SEQ_LENGTH = 323\n",
        "NUM_LABELS = 5\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "label_encoding = {\"B-O\": 1, \"B-AC\": 2, \"B-LF\": 3, \"I-LF\": 4, \"PAD\": 0}\n",
        "filtered_label_encoding = {0:1, 1:2, 3:3, 4:4, \"PAD\":0}\n",
        "\n",
        "def encode_labels(labels):\n",
        "    print(\"Encoding labels...\")\n",
        "    encoded = [[label_encoding[label] for label in sequence] for sequence in labels]\n",
        "    print(\"Labels encoded.\")\n",
        "    return encoded\n",
        "\n",
        "def encode_filtered_labels(labels):\n",
        "    print(\"Encoding labels...\")\n",
        "    encoded = [[filtered_label_encoding[label] for label in sequence] for sequence in labels]\n",
        "    print(\"Labels encoded.\")\n",
        "    return encoded\n",
        "\n",
        "def embed_tokens(tokens):\n",
        "    print(\"Embedding tokens...\")\n",
        "    embedded_tokens = []\n",
        "    for token_sequence in tokens:\n",
        "        sequence_embedding = [word2vec_model[word] if word in word2vec_model else np.zeros(EMBEDDING_DIM) for word in token_sequence]\n",
        "        if len(sequence_embedding) < MAX_SEQ_LENGTH:\n",
        "            sequence_embedding += [np.zeros(EMBEDDING_DIM) for _ in range(MAX_SEQ_LENGTH - len(sequence_embedding))]\n",
        "        embedded_tokens.append(np.array(sequence_embedding[:MAX_SEQ_LENGTH]))\n",
        "    print(\"Tokens embedded.\")\n",
        "    return np.array(embedded_tokens)\n",
        "\n",
        "def pad_sequences_custom(data, maxlen, dtype='float32'):\n",
        "    print(f\"Padding sequences to a maximum length of {maxlen}...\")\n",
        "    padded_data = tf.keras.preprocessing.sequence.pad_sequences(data, maxlen=maxlen, dtype=dtype, padding='post', truncating='post', value=0.0)\n",
        "    print(\"Sequences padded.\")\n",
        "    return padded_data\n",
        "\n",
        "def prepare_data(data, filtered_data):\n",
        "    print(\"Preparing data...\")\n",
        "    tokens = data['tokens']\n",
        "    labels = encode_labels(data['ner_tags'])\n",
        "    embedded_tokens = embed_tokens(tokens)\n",
        "    print(\"Token Embeddings: \", embedded_tokens.shape)\n",
        "    padded_labels = pad_sequences_custom(labels, MAX_SEQ_LENGTH, dtype='int32')\n",
        "    print(\"Data prepared.\")\n",
        "\n",
        "    print(\"Preparing filtered data...\")\n",
        "    clip_size = int(len(filtered_data) * 0.1)\n",
        "    filtered_tokens=filtered_data['tokens'][:clip_size]\n",
        "    filtered_labels=encode_filtered_labels(filtered_data['ner_tags'][:clip_size])\n",
        "    embedded_filtered_tokens=embed_tokens(filtered_tokens)\n",
        "    print(\"Filtered Token Embeddings: \", embedded_filtered_tokens.shape)\n",
        "    padded_filtered_labels = pad_sequences_custom(filtered_labels, MAX_SEQ_LENGTH, dtype='int32')\n",
        "\n",
        "    combined_embeddings = np.concatenate([embedded_tokens, embedded_filtered_tokens], axis=0)\n",
        "    combined_labels = np.concatenate([padded_labels, padded_filtered_labels], axis=0)\n",
        "\n",
        "    return combined_embeddings, to_categorical(combined_labels, num_classes=NUM_LABELS), combined_labels\n",
        "\n",
        "\n",
        "print(\"Preparing training data...\")\n",
        "X_train, y_train, train_padded_labels = prepare_data(train_data, filtered_train_data)\n",
        "sample_weight = np.where(train_padded_labels == label_encoding[\"PAD\"], 0, 1)\n",
        "print(\"Training data ready.\")\n",
        "\n",
        "print(\"Preparing validation data...\")\n",
        "X_val, y_val, val_padded_labels = prepare_data(validation_data, filtered_valid_data)\n",
        "val_sample_weight = np.where(val_padded_labels == label_encoding[\"PAD\"], 0, 1)\n",
        "print(\"Validation data ready.\")\n",
        "\n",
        "print(\"Preparing test data...\")\n",
        "X_test, y_test, test_padded_labels = prepare_data(test_data, filtered_test_data)\n",
        "test_sample_weight = np.where(test_padded_labels == label_encoding[\"PAD\"], 0, 1)\n",
        "print(\"Test data ready.\")\n",
        "\n",
        "def create_bilstm_model(num_labels, lstm_units, vocab_size, embedding_size, w2v_weights, max_sequence_length):\n",
        "    model = Sequential()\n",
        "    model.add(Masking(mask_value=0.0, input_shape=(max_sequence_length, embedding_size)))\n",
        "    model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(num_labels, activation='softmax')))\n",
        "    return model\n",
        "\n",
        "NUM_LABELS_PAD = 5\n",
        "LSTM_UNITS = 64\n",
        "COMBINED_MAX_SEQ_LENGTH = 323\n",
        "w2v_weights = word2vec_model.vectors\n",
        "\n",
        "vocab_size, embedding_size = w2v_weights.shape\n",
        "print(\"Word2Vec weights shape:\", w2v_weights.shape)\n",
        "print(\"Creating LSTM model...\")\n",
        "lstm_model = create_bilstm_model(NUM_LABELS_PAD, LSTM_UNITS, vocab_size, EMBEDDING_DIM, w2v_weights, COMBINED_MAX_SEQ_LENGTH)\n",
        "print(\"LSTM model created.\")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Compile the modelfjl\n",
        "print(\"Before i am pased in: \", X_train.shape)\n",
        "print(\"Starting training...\")\n",
        "history = lstm_model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    sample_weight=sample_weight\n",
        ")\n",
        "print(\"Training completed.\")\n",
        "\n",
        "def evaluate_model(model, X, y):\n",
        "    print(\"Evaluating model...\")\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred = np.argmax(y_pred, axis=-1)\n",
        "    y_true = np.argmax(y, axis=-1)\n",
        "\n",
        "    mask = y_true != label_encoding[\"PAD\"]\n",
        "    y_pred_masked = y_pred[mask]\n",
        "    y_true_masked = y_true[mask]\n",
        "\n",
        "    accuracy = np.mean(y_pred_masked == y_true_masked)\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    labels_without_pad = [label for label in label_encoding if label != \"PAD\"]\n",
        "    label_values_without_pad = [label_encoding[label] for label in labels_without_pad]\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true_masked, y_pred_masked, labels=label_values_without_pad, target_names=labels_without_pad, zero_division=1))\n",
        "\n",
        "    cm = confusion_matrix(y_true_masked, y_pred_masked, labels=label_values_without_pad)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='seismic', xticklabels=labels_without_pad, yticklabels=labels_without_pad)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.savefig('lstmgraphs/confusion_matrix.png')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Evaluating on test data...\")\n",
        "evaluate_model(lstm_model, X_test, y_test)\n",
        "print(\"Evaluation completed.\")"
      ],
      "metadata": {
        "id": "AippdodLd5qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 4: Pre-training LLM's Experimentation"
      ],
      "metadata": {
        "id": "eB88pTANV_9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets\n",
        "%pip install transformers\n",
        "%pip install spacy\n",
        "%pip install torch\n",
        "%pip install spacy-transformers\n",
        "%pip install transformers[torch]\n",
        "%pip install seqeval"
      ],
      "metadata": {
        "id": "bSMmBWvFh4oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6Y-hBGGV4vT"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "if not os.path.exists('llmgraphs'):\n",
        "    os.makedirs('llmgraphs')\n",
        "\n",
        "print('Dataset loaded:', dataset)\n",
        "print('Size of validation set:', len(validation_data))\n",
        "print('Size of test set:', len(test_data))\n",
        "\n",
        "print('First example from the train split:', train_data[0])\n",
        "print('Column names:', train_data.column_names)\n",
        "\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "label_encoding = {\"B-O\": 0, \"B-AC\": 1, \"B-LF\": 2, \"I-LF\": 3}\n",
        "label_list = [\"B-O\", \"B-AC\", \"B-LF\", \"I-LF\"]\n",
        "\n",
        "models_to_test = [\"bert-base-uncased\", \"roberta-base\", \"albert-base-v2\"]\n",
        "results_summary = {}\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
        "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results.get(\"overall_precision\", 0),\n",
        "        \"recall\": results.get(\"overall_recall\", 0),\n",
        "        \"f1\": results.get(\"overall_f1\", 0),\n",
        "        \"accuracy\": results.get(\"overall_accuracy\", 0),\n",
        "    }\n",
        "\n",
        "def tokenize_and_align_labels(examples, tokenizer, label_encoding):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, padding=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None or word_idx == previous_word_idx:\n",
        "                label_ids.append(-100)\n",
        "            else:\n",
        "                label_ids.append(label_encoding[label[word_idx]])\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "models_to_test = [\"bert-base-uncased\", \"roberta-base\", \"albert-base-v2\"]\n",
        "results_summary = {}\n",
        "metrics_history = {model: {'train_loss': [], 'eval_loss': [], 'eval_accuracy': [], 'train_accuracy': []} for model in models_to_test}\n",
        "\n",
        "class LoggingCallback(TrainerCallback):\n",
        "    def _init_(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        model_name = self.model_name  # Use the stored model_name\n",
        "        if 'loss' in logs:\n",
        "            metrics_history[model_name]['train_loss'].append(logs['loss'])\n",
        "        if 'eval_loss' in logs:\n",
        "            metrics_history[model_name]['eval_loss'].append(logs['eval_loss'])\n",
        "        if 'eval_accuracy' in logs:\n",
        "            metrics_history[model_name]['eval_accuracy'].append(logs['eval_accuracy'])\n",
        "        if 'train_accuracy' in logs:\n",
        "            metrics_history[model_name]['train_accuracy'].append(logs['accuracy'])\n",
        "\n",
        "\n",
        "metrics_history = {model: {'train_loss': [], 'eval_loss': [], 'eval_accuracy': [],'train_accuracy': []} for model in models_to_test}\n",
        "\n",
        "for model_name in models_to_test:\n",
        "    print(f\"Processing {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True if \"roberta\" in model_name else False)\n",
        "    tokenized_train_dataset = train_data.map(lambda x: tokenize_and_align_labels(x, tokenizer, label_encoding), batched=True)\n",
        "    tokenized_val_dataset = validation_data.map(lambda x: tokenize_and_align_labels(x, tokenizer, label_encoding), batched=True)\n",
        "\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_encoding))\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./{model_name}-finetuned-NER\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        learning_rate=3e-5,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=32,\n",
        "        num_train_epochs=20,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='f1',\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train_dataset,\n",
        "        eval_dataset=tokenized_val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
        "        callbacks=[LoggingCallback(model_name)]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    results_summary[model_name] = trainer.evaluate(tokenized_val_dataset)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(metrics_history[model_name]['train_loss'], label='Train Loss')\n",
        "    plt.plot(metrics_history[model_name]['eval_loss'], label='Validation Loss')\n",
        "    plt.title(f'{model_name} Training vs Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(metrics_history[model_name]['train_accuracy'], label='Training Accuracy')\n",
        "    plt.plot(metrics_history[model_name]['eval_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'{model_name} Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig(f'llmgraphs/{model_name}_training_validation_metrics.png')\n",
        "    plt.close()\n",
        "\n",
        "    del model, tokenizer, trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "for model_name, results in results_summary.items():\n",
        "    print(f\"\\nFinal Results for {model_name}:\")\n",
        "    for key, value in results.items():\n",
        "        print(f\"{key}: {value:.4f}\")"
      ]
    }
  ]
}